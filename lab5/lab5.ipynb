{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, string, requests, pprint, nltk, operator, numpy as np, math, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_judgment(judgment):\n",
    "    data_to_query = (\",\").join(judgment)\n",
    "    r = requests.post(data=data_to_query.encode(\"utf-8\"), url=\"http://localhost:9200\")\n",
    "    response_text = r.text\n",
    "    splited_response = response_text.splitlines()\n",
    "    splited_response = [\" \".join(x.replace(\"\\t\", \" \").replace(\"none\", \"\")[1:].split(\":\")[:2][:1]).replace(\" \", \":\")\n",
    "                        for x in splited_response if \":\" in x]\n",
    "    return splited_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 1/9 [00:46<06:11, 46.48s/it]\u001b[A\n",
      " 22%|██▏       | 2/9 [00:52<03:02, 26.06s/it]\u001b[A\n",
      " 33%|███▎      | 3/9 [00:59<01:59, 19.90s/it]\u001b[A\n",
      " 44%|████▍     | 4/9 [01:10<01:28, 17.70s/it]\u001b[A\n",
      " 56%|█████▌    | 5/9 [01:37<01:18, 19.58s/it]\u001b[A\n",
      " 67%|██████▋   | 6/9 [02:29<01:14, 24.87s/it]\u001b[A\n",
      " 78%|███████▊  | 7/9 [03:28<00:59, 29.78s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open (file_path) as file:\n",
    "        judgment = []\n",
    "        json_content = json.load(file)\n",
    "        item_count = 0\n",
    "        for item in tqdm.tqdm(json_content):\n",
    "            item_count += 1\n",
    "            text_content = re.sub(\"<.*?>\", \"\", item[\"textContent\"])\n",
    "            text_content = text_content.replace('-\\n', '')\n",
    "            word_content = text_content.split()\n",
    "            topicSpecificPunctuation = '„”–§…«»'\n",
    "            translator = str.maketrans('', '', string.punctuation+topicSpecificPunctuation)\n",
    "\n",
    "            for word in word_content:\n",
    "                word = word.translate(translator).lower()\n",
    "                if len(word)>0:\n",
    "                    judgment.append(word)\n",
    "            unigrams = process_judgment(judgment)\n",
    "                \n",
    "            with open(file_path + \"_results.txt\", 'w') as out_file:\n",
    "                for unigram in unigrams:\n",
    "                    out_file.write(unigram + \"\\n\")\n",
    "            \n",
    "\n",
    "def read_all_judgments_from_2018():\n",
    "    with open(\"../data_filtered/raport.txt\", \"w\") as raport_file:\n",
    "        for filename in tqdm.tqdm(os.listdir(\"../data_filtered/\")):\n",
    "            if not filename + \"_results.txt\" in os.listdir(\"../data_filtered/\"):\n",
    "                raport_file.write(\"Writing to file \" + filename)\n",
    "                read_file(\"../data_filtered/\" + filename)\n",
    "                raport_file.write(\"Writing to file \" + filename + \"FINISHED\")\n",
    "        \n",
    "read_all_judgments_from_2018()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(file_path):\n",
    "    with open(file_path) as file:\n",
    "        content = file.read().split(\"\\n\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_response = processed_unigrams\n",
    "splited_response = [item for sublist in splited_response for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(splited_response)\n",
    "unigrams_frequency = nltk.FreqDist(splited_response)\n",
    "bigrams_frequency = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unigrams = sorted(unigrams_frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "pprint.pprint(sorted_unigrams[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bigrams = sorted(bigrams_frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "pprint.pprint(sorted_bigrams[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOTAL_COUNT = len(splited_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shannon_entrophy(word_occurences, total_words):\n",
    "    sum = 0\n",
    "    for x in np.nditer(word_occurences):\n",
    "        if x!= 0:\n",
    "            sum += (x/total_words) * math.log(x/total_words)\n",
    "    return sum\n",
    "\n",
    "def H(word_occurences):\n",
    "    return shannon_entrophy(word_occurences, TOTAL_COUNT)\n",
    "\n",
    "def calculate_contingency_table(bigram, bigram_count, total_words):\n",
    "    first, second = bigram[0], bigram[1]\n",
    "    first_occurence, second_occurence = unigrams_frequency[first], unigrams_frequency[second]\n",
    "    '''\n",
    "    |------  |---------| \n",
    "    | A,B    |B,notA   |\n",
    "    |------  |---------|\n",
    "    | A,notB |notA,notB|\n",
    "    |------|---------|\n",
    "    '''\n",
    "    return np.array([\n",
    "            [bigram_count, first_occurence-bigram_count],\n",
    "            [second_occurence-bigram_count, total_words-first_occurence-second_occurence-bigram_count]])\n",
    "\n",
    "def log_likeliheood_ratio(bigram_key):\n",
    "    k = calculate_contingency_table(bigram_key, bigrams_frequency[bigram_key], TOTAL_COUNT)\n",
    "    return 2 * np.sum(k) * (H(k) - H(k.sum(axis=0)) -H(k.sum(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ratios = [(key, log_likeliheood_ratio(key), value) for key, value in bigrams_frequency.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_log_ratios = sorted(log_ratios, key=operator.itemgetter(1), reverse=True)\n",
    "sorted_by_log_ratios[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_by_tags = [log_ratio for log_ratio in log_ratios \n",
    "                    if \"subst\" in log_ratio[0][0] and\n",
    "                    (\"subst\" in log_ratio[0][1] or \"adj\" in log_ratio[0][1])]\n",
    "filtered_by_tags_sorted = sorted(filtered_by_tags, key=operator.itemgetter(1), reverse=True)\n",
    "filtered_by_tags_sorted[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
